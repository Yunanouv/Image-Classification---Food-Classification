# -*- coding: utf-8 -*-
"""ImgClassificationDeployment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ScSp1Doy68wNaiy30zcUtZ3nFhc6qAXv

**Image Classification Deployment**
"""

import tensorflow as tf
print(tf.__version__)

from google.colab import drive
drive.mount('/content/drive')

import zipfile,os
local_zip = '/content/drive/MyDrive/Dicoding/Submission 3/Food Classification.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/tmp')
zip_ref.close()

#train dataset
base_path = '/content/drive/MyDrive/Dicoding/Submission 3/kaggle/train'
classes = os.listdir(base_path)
filepaths = []
labels = []
for c in classes:
    flist = os.listdir(base_path + '/' + c)
    for f in flist:
        fpath = os.path.join(base_path, c, f)
        filepaths.append(fpath)
        labels.append(c)
print ('filepaths: ', len(filepaths), '   labels: ', len(labels))

import pandas as pd

Fseries=pd.Series(filepaths, name='file_paths')
Lseries=pd.Series(labels, name='labels')
train_df=pd.concat([Fseries,Lseries], axis=1)
train_df=pd.DataFrame(train_df, columns = ['file_paths', 'labels'])
print(train_df['labels'].value_counts())

#validation dataset
base_path = '/content/drive/MyDrive/Dicoding/Submission 3/kaggle/valid'
classes = os.listdir(base_path)
filepaths = []
labels = []
for c in classes:
    flist = os.listdir(base_path + '/' + c)
    for f in flist:
        fpath = os.path.join(base_path, c, f)
        filepaths.append(fpath)
        labels.append(c)
print ('filepaths: ', len(filepaths), '   labels: ', len(labels))

Fseries=pd.Series(filepaths, name='file_paths')
Lseries=pd.Series(labels, name='labels')
valid_df=pd.concat([Fseries,Lseries], axis=1)
valid_df=pd.DataFrame(valid_df, columns = ['file_paths', 'labels'])
print(valid_df['labels'].value_counts())

#test dataset
base_path = '/content/drive/MyDrive/Dicoding/Submission 3/kaggle/test'
classes = os.listdir(base_path)
filepaths = []
labels = []
for c in classes:
    flist = os.listdir(base_path + '/' + c)
    for f in flist:
        fpath = os.path.join(base_path, c, f)
        filepaths.append(fpath)
        labels.append(c)
print ('filepaths: ', len(filepaths), '   labels: ', len(labels))

Fseries=pd.Series(filepaths, name='file_paths')
Lseries=pd.Series(labels, name='labels')
test_df=pd.concat([Fseries,Lseries], axis=1)
test_df=pd.DataFrame(test_df, columns = ['file_paths', 'labels'])
print(test_df['labels'].value_counts())

import numpy as np
import matplotlib.pyplot as plt

plt.figure(figsize=(14,10))
for i in range(20):
    random = np.random.randint(1,len(train_df))
    plt.subplot(4,5,i+1)
    img = train_df.loc[random,"file_paths"]
    plt.imshow(plt.imread(img))
    plt.title(train_df.loc[random, "labels"], size = 10, color = "black")
    plt.xticks([])
    plt.yticks([])

plt.show()

import keras_preprocessing
from keras_preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications.vgg16 import preprocess_input


target_size=(224,224)
batch_size=64

train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input, zoom_range=0.2, shear_range=0.2, horizontal_flip=True)
test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)

train_gen = train_datagen.flow_from_dataframe(train_df, x_col='file_paths', y_col='labels', target_size=target_size, batch_size=batch_size, color_mode='rgb', class_mode='categorical')
valid_gen = test_datagen.flow_from_dataframe(valid_df, x_col='file_paths', y_col='labels', target_size=target_size, batch_size=batch_size, color_mode='rgb', class_mode='categorical')
test_gen = test_datagen.flow_from_dataframe(test_df, x_col='file_paths', y_col='labels', target_size=target_size, batch_size=batch_size, color_mode='rgb', class_mode='categorical')

base_model = tf.keras.applications.EfficientNetB0(include_top=False, input_shape=(224,224,3), weights='imagenet')
model = tf.keras.Sequential([
    base_model,
    tf.keras.layers.GlobalAveragePooling2D(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(48, activation='softmax')
])

model.summary()

from tensorflow.keras.optimizers import Adam

lr=0.001
model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=lr), metrics=['accuracy'])

# callback
patience = 2
stop_patience = 5
factor = 0.5

callbacks = [
    tf.keras.callbacks.ModelCheckpoint("classify_model.h5", save_best_only=True, verbose = 0),
    tf.keras.callbacks.EarlyStopping(patience=stop_patience, monitor='val_loss', verbose=1, restore_best_weights=True),
    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=factor, patience=patience, verbose=1)
]

epochs = 8
history = model.fit(train_gen, validation_data=valid_gen, epochs=epochs, callbacks=callbacks, verbose=1)

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epoches = range(len(acc))

plt.plot(epoches, acc, 'r', label='Tingkat Akurasi Training')
plt.plot(epoches, val_acc, 'b', label='Tingkat Akurasi Validasi')
plt.title('Akurasi Training dan Validasi')
plt.legend(loc=0)
plt.figure()
plt.show

plt.plot(epoches, loss, 'r', label='Loss Training')
plt.plot(epoches, val_loss, 'b', label='Loss Validasi')
plt.title('Loss Training dan Validasi')
plt.legend(loc=0)
plt.figure()
plt.show

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

with tf.io.gfile.GFile('model.tflite', 'wb') as f:
  f.write(tflite_model)